<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="UTF-8">
	<title>ETL</title>
	<link rel="stylesheet" href="../css/style.css">
	<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css">
	<script
  src="https://code.jquery.com/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>
  <script src="../script/script.js"></script>
</head>
<body>
	<nav class="navbar navbar-expand-lg navbar-light bg-menu-color" style="text-align: center;">
	    <!-- <a class="navbar-brand" href="#">Navbar</a> -->
	    <div class="theTitle"><a href="index.html">ETL</a></div>
	    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavDropdown" aria-controls="navbarNavDropdown" aria-expanded="false" aria-label="Toggle navigation">
	        <span class="navbar-toggler-icon"></span>
	    </button>
	    <div class="collapse navbar-collapse justify-content-end" id="navbarNavDropdown">
	        <ul class="navbar-nav">
	        	 <li class="nav-item">
	            	<a class="nav-link" href="proposal.html">Proposal</a>
	            </li>
	            <li class="nav-item active">
	            	<a class="nav-link" href="report.html">Report</a>
	            </li>
	            <li class="nav-item">
	                <a class="nav-link" href="plots.html">Plots</a>
	            </li>
	            <li class="nav-item">
	                <a class="nav-link" href="data.html">Data</a>
	            </li>   
	        </ul>
	    </div>
	</nav>


	<div class="container">
		<div class="pageTitle">
			<h2>Report</h2>
		</div>
			
	
<h2>The Canadian Landscape For Data Analytics Careers, January 2019</h2>

<h4>Methods</h4>

<p>Using the ETL processes, the following tasks were done:</p>
<ul>
    <li>Extraction of the Data Analytics positions and salaries from job search websites</li>
    <li>Extraction of the Cost of Living information for Canadian cities from Numbeo website</li>
    <li>Extraction of the population information for Canadian locations from Statistics Canada website</li>
    <li>Transformation of the Data Analytics positions data focusing on locations and salary into a dataframe</li>
    <li>Amalgamation and transformation of the Cost of Living and population data into a dataframe based on Canadian Locations and their provinces/territories.</li>
    <li>Transformation Cost of Living and population dataframe to produce a dataframe of aggregated data for each province/territory.</li>
    <li>The Load of all dataframes into a SQL database</li>
    <li>The use of matplotlib for visualization of the relationship between Data Analytics open positions, their locations, and their related cost of living and population information, through database queries</li>
</ul>
<p>The methods are described in greater detail below:</p>




   
<h4>Extraction &amp; Transformation of Data Analytics Jobs, and Salaries</h4>
<p><b>Jobs:</b>The jobs listings were scraped from indeed.ca by searching for 'Data Analyst', 'Data Engineer', and 'Data Scientist' jobs in Canada. A multi-nested loop was created in Python that would extract city, company, job link, job title, location, salary, and company from the most recent 10 pages of listings for each or the three job titles. The script would detect whether the job title contained Analyst, Engineer, Scientist, or non of the above and assign the listing with one of four jobIDs. The script also split location into city and province and created new columns for each. Becuase the postings are dependent on users generating the content, the script had to handle for errors, such as filling location in as Canada rather than city and province, and placing job titles in multiple areas of the post. Variables were set and objects creted inside the loop and appended to an array of job postings outside the loop. Once the job list was created, it was able to be turned into a pandas dataframe for further munging and clean up.</p>

<p><b>Salaries:</b>The salaries for data analytics jobs were obtained by scraping the website indeed.ca/salaries. A loop was constructed to query the website by modifying the url itself, and if a results table was detected, the loop would then scrape all available data from the table using beautiful soup, and continue to the next page of results as long as a &lt;span&gt; tag with a class (&ldquo;cmp-pagination-link&rdquo;) existed. Once results were gathered for all three data science jobs in all provinces, the loop ended and the results were parsed into separate lists for province, job id, job title, salary and a salary classifier (per month, per year etc.). The lists were then used to create a temporary dataframe, which adjusted the salary column to ensure all rows yielded a &lsquo;per/year&rsquo; value. The final dataframe then dropped the classifier column, and was exported for loading subsequent analyses.</p>


<h4>Extraction &amp; Transformation of Location Data</h4>

<p>Whereas career opportunities in the Data Analytics field is key driver for this project, we also needed to gather the quality of life and population data to assess the locations of these opportunities.  The Numbeo website provides information for many cities around the world regarding the cost of living and the quality of life.  Because the locations are world-wide, most of the Canadian cities did not have the province that they were located in.</p>
<p>The Statistics Canada website was a great resource to determine the provinces for each of the city information provided by Numbeo.  Using the 2016 Census, Population and Dwelling Count Highlight Tables, found in the Statistics Canada website, the city names, their provinces, and associated population and dwelling data provided the opportunity to associate cities found in Numbeo with their provinces, but also gave the added dimension of providing population numbers to normalize Numbeo data.</p>
<p>Both sources of data were collected into two dataframes, one based on cities that included cost of living & quality of life indices, and population and dwelling counts, and based on provinces using aggregated measurements of the cities.  Both dataframes were then exported as csv files to be used to load the My SQL database used for this project.
</p>

<h4>Extraction &amp; Transformation of Cost of Living Data from Numbeo</h4>
<p>The Numbeo website provided Cost of Living, Property Prices, Crime, Health Care, Pollution, Quality of Life, and Travel data for locations around the world.  The website provides the means of entering and selecting world cities and displaying their related data above.  Fortunately, Numbeo also provides API functionality if you have been approved and provided an API key.  The project team requested an API-Key for education purposes (free) which was granted and provided the next day.</p>
<p>Extraction first occurred with getting all cities in Numbeo using the following url query:</p>
<p>https://www.numbeo.com/api/cities?api_key=[API_KEY]</p>
<p>The response included a list of cities with a Numbeo city_id as a unique identifier, city name, country name, longitude, and latitude coordinates.  A dataframe was created based on filtering the response for only rows with the country name of ‘Canada’.  The city names provided by Numbeo were not entirely clean as a very small percentage included province codes within the name.  Python was used to split city name to move the province code if it existed to another column.  The script was also adjusted to change Numbeo city names to exactly match the city names listed in the Stats Canada data.</p>

<h4>Extraction &amp; Transformation of Popuation Data from Stats Canada</h4>
<p>Statistics Canada provides a publicly available csv file with the Population and Dwelling Count Highlight Tables from the 2016 Census.  This csv file, located in the repository folder 1_Input as T301EN.CSV can be downloaded at the following url:</p>
<p>https://www12.statcan.gc.ca/census-recensement/2016/dp-pd/hlt-fst/pd-pl/Table.cfm?Lang=Eng&T=301&S=3&O=D</p>
<p>This csv file was extracted as a dataframe and revised to only include population, number of private dwellings, land area (sq. km), population density per sq. km for each location regardless of location type (village, town, city, township, etc.).  All other columns, including French equivalent columns were removed from the dataframe.</p>

<h4>Merging of Cost of Living Population Data based on Canadian Location</h4>
<p>Using the Canadian cities dataframe extracted from Numbeo (see Subsection 2.2.1 above) and the Stats Canada dataframe, both dataframes were left-merged based on the city / location name.  This led to the dropping of any Canadian locations from Statistics Canada data which were not found in the Numbeo list of Canadian Cities.</p>
<p>This also led to the discovery that multiple rows were created because the city name listed from Numbeo had multiple listings in the Stats Canada data.  For example, the name Beaumont are found in Alberta, Newfoundland, and Quebec.  Other discrepancies included Minden being listed as Minden Hills in Numbeo, but as Minden in Statistics Canada.  And some names existed in the same province as both a Town and a Township.  The other issue was the discovery that Numbeo listings were not finding an equivalent location in Statistics Canada data because of accents, this included well known cities as Montreal.  These discrepancies were manually identified, with the purpose of including the correct corresponding Statistics Canada data using the latitude and longitude provided by Numbeo to ensure each row had the correct province assigned.</p>
<p>Additional python code was added to correct these discrepancies for either tables, whether it was to change the name to remove accents or remove spelling discrepancies, and the merge was executed again.
After this left merge, the combined dataframe still had Numbeo rows with no corresponding Statistics Canada data.  Code was then used to assign the provincial code based on the previous manual look up of the geographic coordinates.</p>

<h4>Extraction & Transformation of Cost of Living of Numbeo Locations</h4>
<p>Using the combined dataframe, a loop was used to query Numbeo to get the Cost of Living Data using the Numbeo City ID.  The following query was used:</p>
<p>https://www.numbeo.com/api/city_prices?api_key=[API-KEY]&city_id=[CITY_ID]</p>
<p>From the response, the following data was extracted for each city:</p>
<ul>
	<li>Monthly rent for 3-bedroom apartment in the city centre</li>
	<li>Property price per square feet</li>
	<li>Average monthly net salary</li>
</ul>

<p>These values were added as columns to the combined dataframe.</p>
<p>To include the Cost of Living Index and the Quality of Life Index to these locations, another loop was executed for each row that queried the database based on the city_id:</p>
<p>https://www.numbeo.com/api/indices?api_key=[API-KEY]&city_id=[CITY_ID]</p>
<p>From the response, the following data was extracted for each city:</p>
<ul>
	<li>Cost of Living Index</li>
	<li>Quality of Life Index</li>
</ul>




<h4>FINAL Transformation AND Loading of data to sql database</h4>
<p>Before loading data into SQL, the extracted inputs needed to be transformed relative to each other to ensure clean data is loaded. The file used to do the final transformation and load into SQL is &lsquo;Transform_IntoSQL.jpynb&rsquo;.</p>
<p>The extracted inputs to this file are as follows:</p>
<ul>
    <li>ca scrape for salaries of Data Analyst, Data Scientist, Data Engineer by province in csv form (Salary_extracted_input.csv).</li>
    <li>ca scrape for job postings for Data Analyst, Data Engineer, Data Scientist positions in Canada is csv form (Job_Posting_extracted_input.csv)</li>
    <li>Canadian city information from Stats Canada API and Numbeo API in csv form (complete_city_df.csv)</li>
    <li>Canadian province information from Stats Canada API and Numbeo API in csv form (province_df.csv)</li>
</ul>
<p>The purpose of this section of the project is to create 5 clean tables that are interlinked and loaded onto MySQL. The 5 tables are: Job_Class, Jobs, Cities, Provinces, Salaries.</p>

<h4>Job_Class Table</h4>

<p>The job_Class table was a manually made dataframe to assign a Class_ID (eventually will act as primary key in SQL) to Data Analyst, Data Engineer, and Data Scientist. We included an Other job_Class because the job posting scrap yielded job titles other than Data Analyst, Data Engineer, and Data Scientist, and we did not want to discard them as they had &lsquo;Data&rsquo; in the job title.</p>

<h4>Provinces Table</h4>

<p>The base of the provinces table was made from a pd.read_html scrape of the Wikipedia page for Canadian provinces. It was merged with the province_df.csv on the province abbreviation. Each province was given a province_ID that will act as a primary key in SQL. &lsquo;Other&rsquo; was also included in the provinces table because some of the cities yielded from the job postings scrape of indeed.ca could not be matched to city on the city table and therefore could not be matched to a province.</p>

<h4>Salaries Table</h4>

<p>The salaries table needed to be merged with the job_Class Table and Province Table. Prior to the merges, the provinces column of the raw salaries table (from the Salary_extracted_input.csv) needed to be match the provinces column of the provinces table in terms of string. The python library &lsquo;fuzzy wuzzy&rsquo; (string matching library) was used to accomplish this.</p>

<h4>Cities Table</h4>

<p>The raw city table (from complete_city_df.csv) needed to be merged with the provinces table. Prior to merging, an &lsquo;Other&rsquo; city needed to be created with &lsquo;Other&rsquo; as the province because the jobs table included cities that are not in the city table that will be categories under &lsquo;Other&rsquo;. A unique city_ID is assigned to all the cities in the city table (to act as the primary key in SQL).</p>

<h4>Jobs Table</h4>

<p>The raw data of the jobs table (Job_Posting_extracted_input.csv) included columns city, company, job title, province, and summary. This table needed to be merged with the job_class table, provinces table, and city table. Prior to the merges, several steps had to be performed to ensure clean merges:</p>
<ul>
    <li>Python library unidecode was used on the city column to remove any accents from the French cities (as the city table did not include city names with accents).</li>
    <li>A list of cities that were in the jobs table but not in the city table were generated. The python library &lsquo;fuzzy wuzzy&rsquo; with a score cutoff (80) was used to determine any semantic differences (e.g. Toronto vs. Greater Toronto Area, St. Catherines vs. St. Catharines, Saanich vs. North Saanich). Cities in the jobs table identified to have minor semantic differences to cities in the cities table were changed. All remaining cities in the jobs table that were not in the cities table were categoried under &lsquo;Other&rsquo;.</li>
    <li>The provinces in the jobs table were also corrected with fuzzy wuzzy as mentioned before.</li>
</ul>


<h4>Load onto SQL</h4>

<p>The final dataframes for the job_Class, provinces, cities, salaries, and jobs were loaded onto SQL through python by creating an engine and using the pd.to_sql method. Primary keys were set using the engine.execute method where the SQL query 'ALTER TABLE `table name` ADD PRIMARY KEY (`column name`);' was called.</p>


<h4>FINAL database</h4>

<p>Figure 1 below shows the final database loaded into MySQL. The black lines denote the connections between the tables (i.e. similar to foreign keys).</p>

<div>
	<img src="../images/tablesPic.png" alt="" style="max-width:750px">
</div>


<h4>Findings</h4>

<p>The following data relationships were explored:</p>
<ul>
    <li>The Distribution of Data Analytics Jobs Across Canada</li>
    <li>The Comparison of Data Analytics Average Salaries Across Canada</li>
    <li>The Top 5 Cities with the most Data Analytics Job Opportunites</li>
    <li>The Top 5 Cities with the most Earning Power for Data Analytics Jobs</li>
</ul>
<p>Visualization of these relationships will show the locations in Canada where people desiring a Data Analytics career will have the best opportunities to not only find a job, but to have a higher quality of life.</p>
<h4>Job DistriBution Across Canada</h4>
<p>Ontario, Quebec, and British Columbia (BC) appear to have the highest variety of data science jobs available in the country. In fact, three of these provinces account for &asymp; 90% of the data science jobs in the country. However, when adjusted for population, an interesting pattern emerges. Ontario and BC have nearly the same amount of data science jobs per 100,000 people, while Quebec falls quite a bit shorter. This may suggest that competition in Quebec will be considerably higher than in Ontario and BC.&nbsp;</p>
<p>Interestingly, Alberta, PEI and Sasketchwan all have data science opportunities that are in line with Ontario and BC, however given their slightly higher cost of living index, the salary attained in these provinces should be considerably higher in order to convince an individual to forego pursuing opportunities in Ontario, BC, and Quebec, where the cost of living index is quite a bit lower, presumably because these provinces tend to pay higher salaries than the national average. However, it should be noted that PEI has one of the highest quality of life scores across the country, making it a very viable option</p>

<h4>Average Salaries Across Canada</h4>
<h4>Locations with most opporutnities</h4>
<h4>Locations with most earning power</h4>
<h4>Conclusions</h4>
<h4>Discussion</h4>

<p>The following are items and issues that were encountered during this project, and should be further discussed to provide insight on the observations and conclusions.</p>

<h4>Issues with Job data</h4>
<h4>Incompleteness of Numbeo Data</h4>
<h4>Issues regarding correlating data from different sources</h4>

<h4>Appendix A: Script Execution Plan</h4>
<p>The following are the script designs of the three scripts used in the performance testing.&nbsp; This provides a clear understanding of the steps used when recording the script, the data selected during recording, the variable names used, data preparation instructions, and the LoadRunner transaction names and the functional steps they comprise.</p>
<p>The first script could not be recorded as a LoadRunner script, but the script design document provided clear instructions on how to manually execute the functionality.</p>

<h4>Appendix B: Repository Folder Structure</h4>



</div>
	<div class="footer">
		<p>The Canadian Landscape For Data Analytics Careers by Sara, Laurel, Jose, and Jesse</p>
	</div>


</body>
</html>